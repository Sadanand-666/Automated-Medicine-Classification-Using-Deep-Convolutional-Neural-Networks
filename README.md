# ğŸ§  High-Accuracy Medicine Image Classification Using VGG-19 and Advanced Data Augmentation
This project presents an **end-to-end deep learning pipeline** for **medicine image classification** using **Transfer Learning with VGG-19** in **PyTorch**.  
The system leverages **advanced Albumentations-based image augmentation**, a **custom dataset wrapper**, and a **fine-tuned classifier head** to achieve **high accuracy and strong generalization**.

## ğŸš€ Key Highlights

- Pretrained **VGG-19 (ImageNet weights)**
- Transfer Learning with frozen convolutional layers
- Advanced **Albumentations** data augmentation
- Custom PyTorch Dataset integration
- AdamW optimizer with learning-rate scheduling
- Extensive evaluation metrics & visualizations
- Final **Test Accuracy: 95.01%**

## ğŸ“‚ Dataset Structure

medicine_data/
â”‚
â”œâ”€â”€ Train/

â”‚ â”œâ”€â”€ Class_1/

â”‚ â”œâ”€â”€ Class_2/

â”‚ â””â”€â”€ Class_N/

â”‚

â”‚ â””â”€â”€ Test/

â”‚ â”œâ”€â”€ Class_1/

â”‚ â”œâ”€â”€ Class_2/

â”‚ â””â”€â”€ Class_N/

Each class directory contains labeled medicine images.

## âš™ï¸ Tech Stack

- **Language:** Python  
- **Deep Learning:** PyTorch, Torchvision  
- **Augmentation:** Albumentations, OpenCV  
- **Visualization:** Matplotlib, Seaborn  
- **Metrics:** Scikit-learn  
- **Hardware:** GPU (CUDA supported)  

## ğŸ”„ Data Augmentation Pipeline

Albumentations was used to improve robustness and reduce overfitting.

**Augmentations Applied:**
- Resize & Random Crop  
- Horizontal Flip  
- Affine Transformations (scale, rotate, translate)  
- ImageNet Normalization  

## ğŸ—ï¸ Model Architecture

**Backbone:** VGG-19 (Pretrained on ImageNet)

**Classifier Head:**
Linear (25088 â†’ 512)
ReLU
Dropout (0.4)
Linear (512 â†’ Number of Classes)


**Transfer Learning Strategy:**
- Convolutional layers frozen
- Only classifier layers trained


## âš¡ Training Configuration

| Parameter        | Value |
|------------------|-------|
| Optimizer        | AdamW |
| Learning Rate    | 0.0001 |
| Weight Decay     | 1e-4 |
| Loss Function    | CrossEntropyLoss |
| Scheduler        | StepLR (step=5, gamma=0.5) |
| Batch Size       | 32 |
| Epochs           | 25 |
| Device           | GPU / CPU |


## ğŸ“Š Training Results

- **Training Accuracy:** ~94%  
- **Validation Accuracy:** **95.01%**  
- Smooth convergence  
- Minimal overfitting  
- Stable loss reduction  


## ğŸ“ˆ Evaluation Metrics

The model was evaluated using:

- Accuracy & Loss curves  
- Smoothed learning curves  
- Confusion Matrix  
- Multi-class ROC-AUC curves (One-vs-Rest)  

These metrics provide both **global** and **class-wise** performance insights.

## ğŸ”® Future Improvements

Fine-tuning deeper VGG layers

Grad-CAM visualization for interpretability

EfficientNet / ResNet comparison

Cross-validation

Deployment via FastAPI or TorchServe

## ğŸ“œ Ethical & Usage Disclaimer

This project was developed with conceptual guidance and validation support from ChatGPT, while maintaining a clear understanding of:

CNN architectures

Transfer Learning principles

Data augmentation strategies

Model evaluation techniques

All experimentation, implementation, and analysis were conducted by the author.

# ğŸ‘¤ Author

Sadanand Bhandari

AI & Data Science Practitioner
